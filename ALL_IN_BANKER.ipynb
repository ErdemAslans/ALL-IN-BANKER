{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNJ0my3/BSc42SZEd0Bx+ka",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ErdemAslans/ALL-IN-BANKER/blob/main/ALL_IN_BANKER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy scikit-learn seaborn matplotlib\n",
        "!pip install xgboost lightgbm\n",
        "!pip install optuna\n",
        "!pip install imbalanced-learn\n",
        "!pip install keras tensorflow\n",
        "!pip install plotly dash\n",
        "!pip install joblib\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8X6vpZ76bjBJ",
        "outputId": "6c135bf4-4aab-4182-cf40-413af7cac0f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.1.1)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.10/dist-packages (4.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost) (2.23.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.13.1)\n",
            "Collecting optuna\n",
            "  Downloading optuna-4.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.13.3-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.35)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.5)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.1)\n",
            "Downloading optuna-4.0.0-py3-none-any.whl (362 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.8/362.8 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.13.3-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.2/233.2 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.5 alembic-1.13.3 colorlog-6.8.2 optuna-4.0.0\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.12.4)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.5.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.5.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.9.2)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.11.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.13.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras) (24.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (5.24.1)\n",
            "Collecting dash\n",
            "  Downloading dash-2.18.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly) (9.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly) (24.1)\n",
            "Requirement already satisfied: Flask<3.1,>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from dash) (2.2.5)\n",
            "Requirement already satisfied: Werkzeug<3.1 in /usr/local/lib/python3.10/dist-packages (from dash) (3.0.4)\n",
            "Collecting dash-html-components==2.0.0 (from dash)\n",
            "  Downloading dash_html_components-2.0.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting dash-core-components==2.0.0 (from dash)\n",
            "  Downloading dash_core_components-2.0.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting dash-table==5.0.0 (from dash)\n",
            "  Downloading dash_table-5.0.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from dash) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from dash) (4.12.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from dash) (2.32.3)\n",
            "Collecting retrying (from dash)\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from dash) (1.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from dash) (71.0.4)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash) (2.2.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from Werkzeug<3.1->dash) (3.0.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->dash) (3.20.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->dash) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->dash) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->dash) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->dash) (2024.8.30)\n",
            "Requirement already satisfied: six>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from retrying->dash) (1.16.0)\n",
            "Downloading dash-2.18.1-py3-none-any.whl (7.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dash_core_components-2.0.0-py3-none-any.whl (3.8 kB)\n",
            "Downloading dash_html_components-2.0.0-py3-none-any.whl (4.1 kB)\n",
            "Downloading dash_table-5.0.0-py3-none-any.whl (3.9 kB)\n",
            "Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: dash-table, dash-html-components, dash-core-components, retrying, dash\n",
            "Successfully installed dash-2.18.1 dash-core-components-2.0.0 dash-html-components-2.0.0 dash-table-5.0.0 retrying-1.3.4\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "pzeKkRs2a9uo",
        "outputId": "b7a893ca-9428-45f3-bf54-7b977cef1fa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'give_me_some_credit.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-e774d97af82c>\u001b[0m in \u001b[0;36m<cell line: 97>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-e774d97af82c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_engineering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-e774d97af82c>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'give_me_some_credit.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'give_me_some_credit.csv'"
          ]
        }
      ],
      "source": [
        "######################################\n",
        "# HOME CREDIT DEFAULT RISK COMPETITION XGBoost\n",
        "######################################\n",
        "# Reference from https://www.kaggle.com/jsaguiar/lightgbm-with-simple-features\n",
        "\n",
        "# Importing essential libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import gc\n",
        "from contextlib import contextmanager\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import re\n",
        "from xgboost.callback import EarlyStopping\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 500)\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=UserWarning)\n",
        "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
        "\n",
        "\n",
        "# Defining timer to track progress\n",
        "@contextmanager\n",
        "def timer(title):\n",
        "    t0 = time.time()\n",
        "    yield\n",
        "    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n",
        "\n",
        "\n",
        "# Defining one-hot encoding for categorical columns with get_dummies\n",
        "def one_hot_encoder(df, nan_as_category=True):\n",
        "    original_columns = list(df.columns)\n",
        "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
        "    df = pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category)\n",
        "    new_columns = [c for c in df.columns if c not in original_columns]\n",
        "    return df, new_columns\n",
        "\n",
        "\n",
        "# Defining Sin-cos transformation for cyclic features\n",
        "def encode(df, col, max_val):\n",
        "    df[col + '_SIN'] = np.sin(2 * np.pi * df[col] / max_val)\n",
        "    df[col + '_COS'] = np.cos(2 * np.pi * df[col] / max_val)\n",
        "    return df\n",
        "\n",
        "\n",
        "# Defining dynamic rare encoding for column categories\n",
        "def dyn_rare_encoder(df, columns, rare_percent):\n",
        "    for col in columns:\n",
        "        tmp = df[col].value_counts() / len(df) * 100\n",
        "        rare_labels = tmp[tmp < rare_percent].index\n",
        "        df[col] = np.where(df[col].isin(rare_labels), 'Other', df[col])\n",
        "    return df\n",
        "\n",
        "\n",
        "# Display/plot feature importance\n",
        "def display_importances(feature_importance_df_):\n",
        "    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\n",
        "    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n",
        "    plt.figure(figsize=(8, 10))\n",
        "    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n",
        "    plt.title('Model Features (avg over folds)')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('model_importances.png')\n",
        "    plt.close()\n",
        "\n",
        "#####################################\n",
        "# Application Train and Test Data\n",
        "#####################################\n",
        "def application_train_test(num_rows=None, nan_as_category=True):\n",
        "    # Read and merge data\n",
        "    df = pd.read_csv('/content/drive/MyDrive/Risk/application_train.csv', nrows=num_rows)\n",
        "    test_df = pd.read_csv('/content/drive/MyDrive/Risk/application_test.csv', nrows=num_rows)\n",
        "    print(\"Train samples: {}, test samples: {}\".format(len(df), len(test_df)))\n",
        "\n",
        "    # Concatenate dataframes\n",
        "    df = pd.concat([df, test_df], ignore_index=True)\n",
        "\n",
        "    # Removing 4 applications with XNA CODE_GENDER (train set)\n",
        "    df = df[df['CODE_GENDER'] != 'XNA']\n",
        "\n",
        "    # Deleting FLAG_MOBIL because there is only 1 person without mobile phone\n",
        "    df.drop('FLAG_MOBIL', axis=1, inplace=True)\n",
        "    df.drop('FLAG_CONT_MOBILE', axis=1, inplace=True)\n",
        "\n",
        "    # NaN values for DAYS_EMPLOYED: 365243 -> nan\n",
        "    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True)\n",
        "\n",
        "    # Changing rare categories of NAME_INCOME_TYPE with the similar categories\n",
        "    df.loc[df['NAME_INCOME_TYPE'] == 'Student', 'NAME_INCOME_TYPE'] = 'State servant'\n",
        "    df.loc[df['NAME_INCOME_TYPE'] == 'Maternity leave', 'NAME_INCOME_TYPE'] = 'Pensioner'\n",
        "    df.loc[df['NAME_INCOME_TYPE'] == 'Unemployed', 'NAME_INCOME_TYPE'] = 'Pensioner'\n",
        "    df.loc[df['NAME_INCOME_TYPE'] == 'Businessman', 'NAME_INCOME_TYPE'] = 'Commercial associate'\n",
        "\n",
        "    # Dynamic rare encoding\n",
        "    df = dyn_rare_encoder(df, ['ORGANIZATION_TYPE'], rare_percent=1.9)\n",
        "    df = dyn_rare_encoder(df, ['NAME_TYPE_SUITE'], rare_percent=3.6)\n",
        "    df = dyn_rare_encoder(df, ['OCCUPATION_TYPE'], rare_percent=1.5)\n",
        "    df = dyn_rare_encoder(df, ['WALLSMATERIAL_MODE'], rare_percent=20)\n",
        "\n",
        "    # Rare Encoding NAME_HOUSING_TYPE with 'Other'\n",
        "    df.loc[(df['NAME_HOUSING_TYPE'] == 'Office apartment') &\n",
        "           (df['NAME_HOUSING_TYPE'] == 'Co-op apartment'), 'NAME_HOUSING_TYPE'] = 'Other'\n",
        "\n",
        "    #  Changing unknown family status with the most observed category\n",
        "    df['NAME_FAMILY_STATUS'].replace('Unknown', 'Married', inplace=True)\n",
        "\n",
        "    #  Changing HOUSETYPE_MODE not null values with\n",
        "    df.loc[df['HOUSETYPE_MODE'].notnull(), 'HOUSETYPE_MODE'] = 'house_type_reported'\n",
        "\n",
        "    # Changing weekdays with integer values\n",
        "    weekday_dict = {'MONDAY': 1, 'TUESDAY': 2, 'WEDNESDAY': 3, 'THURSDAY': 4, 'FRIDAY': 5, 'SATURDAY': 6, 'SUNDAY': 7}\n",
        "    df.replace({'WEEKDAY_APPR_PROCESS_START': weekday_dict}, inplace=True)\n",
        "    # Creating sin-cos transformed features\n",
        "    df = encode(df, 'WEEKDAY_APPR_PROCESS_START', 7)\n",
        "    df = encode(df, 'HOUR_APPR_PROCESS_START', 23)\n",
        "    # Deleting initial WEEKDAY_APPR_PROCESS_START and HOUR_APPR_PROCESS_START features\n",
        "    df.drop(['WEEKDAY_APPR_PROCESS_START', 'HOUR_APPR_PROCESS_START'], axis=1, inplace=True)\n",
        "\n",
        "    # New features (percentages)\n",
        "    df['NEW_DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
        "    df['NEW_INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']\n",
        "    df['NEW_INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n",
        "    df['NEW_ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
        "    df['NEW_ANNUITY_CREDIT_RATIO'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']\n",
        "\n",
        "    # Loan to Value Ratio (LVR)\n",
        "    df['NEW_LVR'] = df['AMT_CREDIT'] / df['AMT_GOODS_PRICE']\n",
        "\n",
        "    # LVR_RISK assesment feature\n",
        "    df.loc[df['NEW_LVR'] >= 0.80, 'NEW_LVR_RISK'] = 1\n",
        "    df.loc[df['NEW_LVR'] < 0.80, 'NEW_LVR_RISK'] = 0\n",
        "\n",
        "    # Mean of External Sources\n",
        "    df[\"NEW_EXT_MEAN\"] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n",
        "\n",
        "    # Product of External Sources\n",
        "    df['NEW_EXT_PROD'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2'] * df['EXT_SOURCE_3']\n",
        "\n",
        "    # Ages of customers\n",
        "    df['NEW_AGE'] = df['DAYS_BIRTH'] / -365\n",
        "\n",
        "    # NEW_AGE_SEGMENT segments\n",
        "    df.loc[df['NEW_AGE'] <= 34, 'NEW_AGE_SEGMENT'] = 'AGE_GRP_1'\n",
        "    df.loc[(df['NEW_AGE'] > 34) & (df['NEW_AGE'] <= 54), 'NEW_AGE_SEGMENT'] = 'AGE_GRP_2'\n",
        "    df.loc[df['NEW_AGE'] > 54, 'NEW_AGE_SEGMENT'] = 'AGE_GRP_3'\n",
        "\n",
        "    # Total documents demonstrated\n",
        "    df['NEW_TOTAL_DOC_NUM'] = df.loc[:, 'FLAG_DOCUMENT_2':'FLAG_DOCUMENT_21'].sum(axis=1)\n",
        "    df.drop(df.loc[:, 'FLAG_DOCUMENT_2':'FLAG_DOCUMENT_21'], axis=1, inplace=True)\n",
        "\n",
        "    # Product-Credit-Salary relation\n",
        "    df[\"NEW_PROD_CRED_SALARY\"] = (df[\"AMT_GOODS_PRICE\"] - df[\"AMT_CREDIT\"]) / df[\"AMT_INCOME_TOTAL\"]\n",
        "\n",
        "    # NEW_ACCOMPANIED feature\n",
        "    df.loc[df['NAME_TYPE_SUITE'] == 'Unaccompanied', 'NEW_ACCOMPANIED'] = 0\n",
        "    df.loc[df['NAME_TYPE_SUITE'] != 'Unaccompanied', 'NEW_ACCOMPANIED'] = 1\n",
        "    df.loc[df['NAME_TYPE_SUITE'].isnull(), 'NEW_ACCOMPANIED'] = np.nan\n",
        "\n",
        "    # Social circle with both 30 and 60 days default (binary)\n",
        "    df.loc[(df['DEF_30_CNT_SOCIAL_CIRCLE'] > 0) & (df['DEF_60_CNT_SOCIAL_CIRCLE'] > 0),\n",
        "           'NEW_DEF_30&60_SOCIAL_CIRCLE'] = 1\n",
        "    df.loc[(df['DEF_30_CNT_SOCIAL_CIRCLE'] == 0) & (df['DEF_60_CNT_SOCIAL_CIRCLE'] == 0),\n",
        "           'NEW_DEF_30&60_SOCIAL_CIRCLE'] = 0\n",
        "\n",
        "    # Label encoding\n",
        "    for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n",
        "        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n",
        "\n",
        "    # Categorical features with One-Hot encode\n",
        "    df, cat_cols = one_hot_encoder(df, nan_as_category)\n",
        "    del test_df\n",
        "    gc.collect()\n",
        "    return df\n",
        "\n",
        "\n",
        "#####################################\n",
        "# Bureau Data\n",
        "#####################################\n",
        "def bb__agg(num_rows=None, nan_as_category=True):\n",
        "    bb = pd.read_csv('/content/drive/MyDrive/Risk/bureau_balance.csv', nrows=num_rows)\n",
        "\n",
        "\n",
        "    # DPD (Days Past Due) 'ye düşmüşmü, düşmemişmi?'\n",
        "    liste = ['1', '2', '3', '4', '5']\n",
        "    bb['NEW_FLAG'] = bb['STATUS'].apply(lambda x: 1 if (x in liste) else (\"X\" if x == \"X\" else 0))\n",
        "\n",
        "    bb, bb_cat = one_hot_encoder(bb, nan_as_category)\n",
        "    bb.drop(\"NEW_FLAG_X\", inplace=True, axis=1)\n",
        "    bb_cat.remove('NEW_FLAG_X')\n",
        "\n",
        "    bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size']}\n",
        "    for col in bb_cat:\n",
        "        bb_aggregations[col] = ['mean']\n",
        "\n",
        "    bb_agg = bb.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n",
        "    bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n",
        "\n",
        "    del bb\n",
        "    gc.collect()\n",
        "    return bb_agg, bb_cat\n",
        "\n",
        "\n",
        "def bureau_(num_rows=None, nan_as_category=True):\n",
        "    bu = pd.read_csv('/content/drive/MyDrive/Risk/bureau.csv', nrows=num_rows)\n",
        "\n",
        "\n",
        "    # Kredi Aktive ve Closed toplam Sayılarını ve Oranlarını hesaplamak\n",
        "    temp_bu = bu[['SK_ID_CURR', 'CREDIT_ACTIVE']]\n",
        "    temp_bu = pd.get_dummies(temp_bu)\n",
        "    temp_bu = temp_bu.groupby('SK_ID_CURR').agg({'CREDIT_ACTIVE_Active': 'sum', 'CREDIT_ACTIVE_Closed': 'sum'})\n",
        "    temp_bu.columns = ['CREDIT_ACTIVE_Active_Count', 'CREDIT_ACTIVE_Closed_Count']\n",
        "    temp_bu['CREDIT_ACTIVE_Active_ratio'] = temp_bu['CREDIT_ACTIVE_Active_Count'] / (\n",
        "                temp_bu['CREDIT_ACTIVE_Active_Count'] + temp_bu['CREDIT_ACTIVE_Closed_Count'])\n",
        "    temp_bu['CREDIT_ACTIVE_Closed_ratio'] = temp_bu['CREDIT_ACTIVE_Closed_Count'] / (\n",
        "                temp_bu['CREDIT_ACTIVE_Active_Count'] + temp_bu['CREDIT_ACTIVE_Closed_Count'])\n",
        "    bu = bu.merge(temp_bu, on=['SK_ID_CURR'], how='left')\n",
        "\n",
        "    # Kredi DAYS_CREDIT'i SK_ID_CURR bazında sıralayarak NEW_DAYS_DIFF değişkeni üretmek kredi alma frekansı bilgisi verebilir.\n",
        "    temp_bu = bu[['SK_ID_CURR', 'SK_ID_BUREAU', 'DAYS_CREDIT']].groupby(by=['SK_ID_CURR'], as_index=False).apply(\n",
        "      lambda x: x.sort_values(['DAYS_CREDIT'], ascending=True)).reset_index(drop=True)\n",
        "    temp_bu['NEW_DAYS_DIFF'] = temp_bu.groupby(by=['SK_ID_CURR'])['DAYS_CREDIT'].diff()\n",
        "    temp_bu = temp_bu[['SK_ID_BUREAU', 'NEW_DAYS_DIFF']]\n",
        "    temp_bu['NEW_DAYS_DIFF'] = temp_bu['NEW_DAYS_DIFF'].fillna(0)\n",
        "    bu = bu.merge(temp_bu, on=['SK_ID_BUREAU'], how='left')\n",
        "\n",
        "    # Active ve Closed Krediler için kredi erken kapanmışmı?\n",
        "    bu.loc[(bu['CREDIT_ACTIVE'] == 'Active') & (bu['DAYS_CREDIT_ENDDATE'] < 0), 'NEW_EARLY_ACTİVE'] = 1\n",
        "    bu.loc[(bu['CREDIT_ACTIVE'] == 'Closed') & (\n",
        "                abs(bu['DAYS_CREDIT_ENDDATE']) < abs(bu['DAYS_ENDDATE_FACT'])), 'NEW_EARLY_CLOSED'] = 1\n",
        "\n",
        "    # Uzatılmış Kredilerin 1 ile değiştirilmesi\n",
        "    prolong = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "    bu['CNT_CREDIT_PROLONG'].replace(prolong, 1, inplace=True)\n",
        "\n",
        "    # Kişi Kaç farklı kredi tipi almış\n",
        "    temp_bu = bu[['SK_ID_CURR', 'CREDIT_TYPE']].groupby(by=['SK_ID_CURR'])[\n",
        "        'CREDIT_TYPE'].nunique().reset_index().rename(index=str, columns={'CREDIT_TYPE': 'NEW_BUREAU_LOAN_TYPES'})\n",
        "    bu = bu.merge(temp_bu, on=['SK_ID_CURR'], how='left')\n",
        "\n",
        "    # Borç Oranı\n",
        "    bu['NEW_DEPT_RATİO'] = bu['AMT_CREDIT_SUM_DEBT'] / (bu['AMT_CREDIT_SUM'] + 1)\n",
        "\n",
        "    # Kredi Tiplerinin 'others' ile değiştirilmesi\n",
        "    credit_type = ['Loan for working capital replenishment',\n",
        "                   'Loan for business development', 'Real estate loan',\n",
        "                   'Unknown type of loan', 'Another type of loan',\n",
        "                   'Cash loan (non-earmarked)', 'Loan for the purchase of equipment',\n",
        "                   'Mobile operator loan', 'Interbank credit',\n",
        "                   'Loan for purchase of shares (margin lending)']\n",
        "\n",
        "    bu['CREDIT_TYPE'].replace(credit_type, 'others', inplace=True)\n",
        "\n",
        "    # Aylık Ödeme Oranı\n",
        "    bu['NEW_AMT_ANNUITY_RATİO'] = bu['AMT_ANNUITY'] / bu['AMT_CREDIT_SUM']\n",
        "\n",
        "    # Kredi güncellenmesi yenimi ?\n",
        "    bu['NEWS_DAYS_CREDIT_UPDATE'] = bu['DAYS_CREDIT_UPDATE'].apply(lambda x: 'old' if x < -90 else 'new')\n",
        "\n",
        "    # 'CREDIT_CURRENCY' değişkenini düşürmek\n",
        "    bu.drop('CREDIT_CURRENCY', inplace=True, axis=1)\n",
        "\n",
        "    del temp_bu\n",
        "    gc.collect()\n",
        "    return bu\n",
        "\n",
        "\n",
        "def combine(bureau, bb_agg):\n",
        "    bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n",
        "    bureau.drop(['SK_ID_BUREAU'], axis=1, inplace=True)\n",
        "    return bureau\n",
        "\n",
        "\n",
        "# Preprocess bureau.csv and bureau_balance.csv\n",
        "def bureau_and_balance(num_rows=None, nan_as_category=True):\n",
        "    bb_agg, bb_cat = bb__agg(num_rows, nan_as_category)\n",
        "    bureau = bureau_(num_rows, nan_as_category)\n",
        "    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category)\n",
        "    bureau = combine(bureau, bb_agg)\n",
        "\n",
        "    # Bureau and bureau_balance numeric features\n",
        "    num_aggregations = {\n",
        "        'DAYS_CREDIT': ['min', 'max', 'mean', 'var'],\n",
        "        'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n",
        "        'DAYS_CREDIT_UPDATE': ['mean'],\n",
        "        'CREDIT_DAY_OVERDUE': ['max', 'mean'],\n",
        "        'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n",
        "        'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
        "        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n",
        "        'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
        "        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n",
        "        'AMT_ANNUITY': ['max', 'mean'],\n",
        "        'CNT_CREDIT_PROLONG': ['sum'],\n",
        "        'MONTHS_BALANCE_MIN': ['min'],\n",
        "        'MONTHS_BALANCE_MAX': ['max'],\n",
        "        'MONTHS_BALANCE_SIZE': ['mean', 'sum'],\n",
        "        \"CREDIT_ACTIVE_Active_Count\": [\"mean\"],\n",
        "        \"CREDIT_ACTIVE_Closed_Count\": [\"mean\"],\n",
        "        \"CREDIT_ACTIVE_Active_ratio\": [\"mean\"],\n",
        "        \"NEW_DAYS_DIFF\": ['max', 'mean'],\n",
        "        \"NEW_EARLY_ACTİVE\": ['mean'],\n",
        "        \"NEW_EARLY_CLOSED\": ['mean'],\n",
        "        \"NEW_BUREAU_LOAN_TYPES\": ['mean'],\n",
        "        \"NEW_DEPT_RATİO\": ['max', 'mean'],\n",
        "        \"NEW_AMT_ANNUITY_RATİO\": ['max', 'mean']\n",
        "    }\n",
        "\n",
        "    for col in bb_cat:\n",
        "        num_aggregations[col + \"_MEAN\"] = ['mean']\n",
        "\n",
        "    # Bureau and bureau_balance categorical features\n",
        "    cat_aggregations = {}\n",
        "    for cat in bureau_cat: cat_aggregations[cat] = ['mean']\n",
        "    for cat in bb_cat: cat_aggregations[cat + \"_MEAN\"] = ['mean']\n",
        "\n",
        "    bureau_agg = bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
        "    bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n",
        "\n",
        "    # Bureau: Active credits - using only numerical aggregations\n",
        "    active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n",
        "    active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n",
        "    active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n",
        "    bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\n",
        "\n",
        "    del active, active_agg\n",
        "    gc.collect()\n",
        "\n",
        "    # Bureau: Closed credits - using only numerical aggregations\n",
        "    closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n",
        "    closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n",
        "    closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n",
        "    bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n",
        "\n",
        "    del closed, closed_agg, bureau, bb_agg\n",
        "    gc.collect()\n",
        "    return bureau_agg\n",
        "\n",
        "\n",
        "#####################################\n",
        "# Previous Application Data\n",
        "#####################################\n",
        "def previous_app(num_rows=None, nan_as_category=True):\n",
        "    df_prev = pd.read_csv('/content/drive/MyDrive/Risk/previous_application.csv', nrows=num_rows)\n",
        "    cat_cols = [col for col in df_prev.columns if df_prev[col].dtypes == 'O']\n",
        "    num_cols = [col for col in df_prev.columns if df_prev[col].dtypes != 'O']\n",
        "\n",
        "    # days 365243 values to nan\n",
        "    df_prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace=True)\n",
        "    df_prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace=True)\n",
        "    df_prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace=True)\n",
        "    df_prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace=True)\n",
        "    df_prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace=True)\n",
        "\n",
        "    # XNA, XAP to nan for cat_cols.\n",
        "    na = ['XNA', 'XAP']\n",
        "    for col in cat_cols:\n",
        "        for n in na:\n",
        "            df_prev.loc[df_prev[col] == n, col] = np.nan\n",
        "\n",
        "    # delete columns columns that do not contain information or missing values over 80 percent of the entire data\n",
        "    del_cols = ['RATE_INTEREST_PRIMARY', 'RATE_INTEREST_PRIVILEGED', 'DAYS_FIRST_DRAWING',\n",
        "                'NAME_CASH_LOAN_PURPOSE', 'CODE_REJECT_REASON', 'FLAG_LAST_APPL_PER_CONTRACT',\n",
        "                'NFLAG_LAST_APPL_IN_DAY', 'SELLERPLACE_AREA']\n",
        "    df_prev.drop(del_cols, axis=1, inplace=True)\n",
        "\n",
        "    # Feature Engineering\n",
        "    # X-sell approved & Walk-in Approved\n",
        "    df_prev['NEW_X_SELL_APPROVED'] = 0\n",
        "    df_prev.loc[(df_prev['NAME_PRODUCT_TYPE'] == 'x-sell') &\n",
        "                (df_prev['NAME_CONTRACT_STATUS'] == 'Approved'), 'NEW_X_SELL_APPROVED'] = 1\n",
        "    df_prev['NEW_WALK_IN_APPROVED'] = 0\n",
        "    df_prev.loc[(df_prev['NAME_PRODUCT_TYPE'] == 'walk-in') &\n",
        "                (df_prev['NAME_CONTRACT_STATUS'] == 'Approved'), 'NEW_WALK_IN_APPROVED'] = 1\n",
        "\n",
        "    # Customer status approved\n",
        "    df_prev['NEW_REPEATER_APPROVED'] = 0\n",
        "    df_prev.loc[(df_prev['NAME_CLIENT_TYPE'] == 'Repeater') &\n",
        "                (df_prev['NAME_CONTRACT_STATUS'] == 'Approved'), 'NEW_REPEATER_APPROVED'] = 1\n",
        "    df_prev['NEW_NEWCUST_APPROVED'] = 0\n",
        "    df_prev.loc[(df_prev['NAME_CLIENT_TYPE'] == 'New') &\n",
        "                (df_prev['NAME_CONTRACT_STATUS'] == 'Approved'), 'NEW_NEWCUST_APPROVED'] = 1\n",
        "    df_prev['NEW_REFRESHED_APPROVED'] = 0\n",
        "    df_prev.loc[(df_prev['NAME_CLIENT_TYPE'] == 'Refreshed') &\n",
        "                (df_prev['NAME_CONTRACT_STATUS'] == 'Approved'), 'NEW_REFRESHED_APPROVED'] = 1\n",
        "\n",
        "    # Purpose of application approved\n",
        "    df_prev['NEW_CARDS_APPROVED'] = 0\n",
        "    df_prev.loc[(df_prev['NAME_PORTFOLIO'] == 'Cards') &\n",
        "                (df_prev['NAME_CONTRACT_STATUS'] == 'Approved'), 'NEW_CARDS_APPROVED'] = 1\n",
        "    df_prev['NEW_CASH_APPROVED'] = 0\n",
        "    df_prev.loc[(df_prev['NAME_PORTFOLIO'] == 'Cash') &\n",
        "                (df_prev['NAME_CONTRACT_STATUS'] == 'Approved'), 'NEW_CASH_APPROVED'] = 1\n",
        "    df_prev['NEW_POS_APPROVED'] = 0\n",
        "    df_prev.loc[(df_prev['NAME_PORTFOLIO'] == 'POS') &\n",
        "                (df_prev['NAME_CONTRACT_STATUS'] == 'Approved'), 'NEW_POS_APPROVED'] = 1\n",
        "\n",
        "    # Interest approved\n",
        "    df_prev['NEW_HIGH_APPROVED'] = 0\n",
        "    df_prev.loc[(df_prev['NAME_YIELD_GROUP'] == 'high') &\n",
        "                (df_prev['NAME_CONTRACT_STATUS'] == 'Approved'), 'NEW_HIGH_APPROVED'] = 1\n",
        "    df_prev['NEW_MIDDLE_APPROVED'] = 0\n",
        "    df_prev.loc[(df_prev['NAME_YIELD_GROUP'] == 'middle') &\n",
        "                (df_prev['NAME_CONTRACT_STATUS'] == 'Approved'), 'NEW_MIDDLE_APPROVED'] = 1\n",
        "    df_prev['NEW_LOWACTION_APPROVED'] = 0\n",
        "    df_prev.loc[(df_prev['NAME_YIELD_GROUP'] == 'low_action') &\n",
        "                (df_prev['NAME_CONTRACT_STATUS'] == 'Approved'), 'NEW_LOWACTION_APPROVED'] = 1\n",
        "    df_prev['NEW_LOWNORMAL_APPROVED'] = 0\n",
        "    df_prev.loc[(df_prev['NAME_YIELD_GROUP'] == 'low_normal') &\n",
        "                (df_prev['NAME_CONTRACT_STATUS'] == 'Approved'), 'NEW_LOWNORMAL_APPROVED'] = 1\n",
        "\n",
        "    # Application hour convert to categorical\n",
        "    df_prev.loc[(df_prev['HOUR_APPR_PROCESS_START'] >= 0) &\n",
        "                (df_prev['HOUR_APPR_PROCESS_START'] <= 6), 'NEW_APP_DAY_TIME'] = 'night'\n",
        "    df_prev.loc[(df_prev['HOUR_APPR_PROCESS_START'] > 6) &\n",
        "                (df_prev['HOUR_APPR_PROCESS_START'] <= 12), 'NEW_APP_DAY_TIME'] = 'morning'\n",
        "    df_prev.loc[(df_prev['HOUR_APPR_PROCESS_START'] > 12) &\n",
        "                (df_prev['HOUR_APPR_PROCESS_START'] <= 18), 'NEW_APP_DAY_TIME'] = 'afternoon'\n",
        "    df_prev.loc[(df_prev['HOUR_APPR_PROCESS_START'] > 18) &\n",
        "                (df_prev['HOUR_APPR_PROCESS_START'] < 24), 'NEW_APP_DAY_TIME'] = 'evening'\n",
        "    df_prev.drop('HOUR_APPR_PROCESS_START', axis=1, inplace=True)\n",
        "\n",
        "    # Client apply with someone\n",
        "    df_prev.loc[df_prev['NAME_TYPE_SUITE'] == 'Unaccompanied', 'NEW_ACCOMPANIED'] = 0\n",
        "    df_prev.loc[df_prev['NAME_TYPE_SUITE'] != 'Unaccompanied', 'NEW_ACCOMPANIED'] = 1\n",
        "    df_prev.loc[df_prev['NAME_TYPE_SUITE'].isnull(), 'NEW_ACCOMPANIED'] = np.nan\n",
        "    df_prev.drop('NAME_TYPE_SUITE', axis=1, inplace=True)\n",
        "\n",
        "    # credit requested / credit given ratio\n",
        "    df_prev['NEW_APP_CREDIT_RATIO'] = df_prev['AMT_APPLICATION'].div(df_prev['AMT_CREDIT']).replace(np.inf, 0)\n",
        "    # loan installment / credit amount ratio\n",
        "    df_prev['NEW_ANNUITY_CREDIT_RATIO'] = df_prev['AMT_ANNUITY'] / df_prev['AMT_CREDIT']\n",
        "    # credit amount / goods price ratio\n",
        "    df_prev['NEW_CREDIT_GOODS_RATIO'] = df_prev['AMT_CREDIT'].div(df_prev['AMT_GOODS_PRICE']).replace(np.inf, 0)\n",
        "    # interest amount\n",
        "    df_prev['NEW_AMT_INTEREST'] = df_prev['CNT_PAYMENT'] * df_prev['AMT_ANNUITY'] - df_prev['AMT_CREDIT']\n",
        "    # interest ratio\n",
        "    df_prev['NEW_INTEREST_RATIO'] = df_prev['NEW_AMT_INTEREST'] / df_prev['AMT_CREDIT']\n",
        "    # needed amount / credit amount (belki silinir)\n",
        "    df_prev['NEW_AMT_NEEDED_CREDIT_RATIO'] = (df_prev['AMT_GOODS_PRICE'] - df_prev['AMT_DOWN_PAYMENT']) / \\\n",
        "                                             df_prev['AMT_CREDIT']\n",
        "\n",
        "    # risk assessment via NEW_CREDIT_GOODS_RATIO\n",
        "    df_prev.loc[df_prev['NEW_CREDIT_GOODS_RATIO'] >= 0.80, 'NEW_CREDIT_GOODS_RISK'] = 1\n",
        "    df_prev.loc[df_prev['NEW_CREDIT_GOODS_RATIO'] < 0.80, 'NEW_CREDIT_GOODS_RISK'] = 0\n",
        "\n",
        "    # risk to approved\n",
        "    df_prev['NEW_RISK_APPROVED'] = 0\n",
        "    df_prev.loc[(df_prev['NEW_CREDIT_GOODS_RISK'] == 1) &\n",
        "                (df_prev['NAME_CONTRACT_STATUS'] == 'Approved'), 'NEW_RISK_APPROVED'] = 1\n",
        "\n",
        "    # non risk to approved\n",
        "    df_prev['NEW_NONRISK_APPROVED'] = 0\n",
        "    df_prev.loc[(df_prev['NEW_CREDIT_GOODS_RISK'] == 0) &\n",
        "                (df_prev['NAME_CONTRACT_STATUS'] == 'Approved'), 'NEW_NONRISK_APPROVED'] = 1\n",
        "\n",
        "    # Application weekdays cycle encoding\n",
        "    df_prev['WEEKDAY_APPR_PROCESS_START'] = df_prev['WEEKDAY_APPR_PROCESS_START'].map({\n",
        "        'MONDAY': 1, 'TUESDAY': 2, 'WEDNESDAY': 3, 'THURSDAY': 4, 'FRIDAY': 5, 'SATURDAY': 6, 'SUNDAY': 7})\n",
        "    df_prev['NEW_WEEKDAY_SIN'] = np.sin(2 * np.pi * df_prev['WEEKDAY_APPR_PROCESS_START'] / 7)\n",
        "    df_prev['NEW_WEEKDAY_COS'] = np.cos(2 * np.pi * df_prev['WEEKDAY_APPR_PROCESS_START'] / 7)\n",
        "    df_prev.drop('WEEKDAY_APPR_PROCESS_START', axis=1, inplace=True)\n",
        "\n",
        "    # Rare encoding\n",
        "    a = ['Auto Accessories', 'Jewelry', 'Homewares', 'Medical Supplies', 'Vehicles', 'Sport and Leisure',\n",
        "         'Gardening', 'Other', 'Office Appliances', 'Tourism', 'Medicine', 'Direct Sales', 'Fitness',\n",
        "         'Additional Service', 'Education', 'Weapon', 'Insurance', 'House Construction', 'Animals']\n",
        "    df_prev[\"NAME_GOODS_CATEGORY\"] = df_prev[\"NAME_GOODS_CATEGORY\"].replace(a, 'others')\n",
        "\n",
        "    b = ['Channel of corporate sales', 'Car dealer']\n",
        "    df_prev[\"CHANNEL_TYPE\"] = df_prev[\"CHANNEL_TYPE\"].replace(b, 'Other_Channel')\n",
        "\n",
        "    c = ['Auto technology', 'Jewelry', 'MLM partners', 'Tourism']\n",
        "    df_prev[\"NAME_SELLER_INDUSTRY\"] = df_prev[\"NAME_SELLER_INDUSTRY\"].replace(c, 'Others')\n",
        "\n",
        "    d = ['Non-cash from your account', 'Cashless from the account of the employer']\n",
        "    df_prev[\"NAME_PAYMENT_TYPE\"] = df_prev[\"NAME_SELLER_INDUSTRY\"].replace(d, 'Others')\n",
        "\n",
        "    # One hot encoder\n",
        "    new_df_prev, new_cat_cols = one_hot_encoder(df_prev, nan_as_category)\n",
        "\n",
        "    # Getting to all the cat cols\n",
        "    origin_bin_cols = [col for col in df_prev.columns if (df_prev[col].dtypes != 'O') & (df_prev[col].nunique() == 2)]\n",
        "    all_cat_cols = new_cat_cols + origin_bin_cols\n",
        "\n",
        "    # Getting to the num cols\n",
        "    # x_cols = ['SK_ID_PREV','SK_ID_CURR', 'DAYS_FIRST_DUE','DAYS_LAST_DUE_1ST_VERSION','DAYS_LAST_DUE','DAYS_TERMINATION']\n",
        "    # new_num_cols = [col for col in new_df.columns if (col not in all_binary_cols) and (col not in x_cols)]\n",
        "    # num_aggregations = {}\n",
        "    # for num in new_num_cols:\n",
        "    # num_aggregations[num] = ['min', 'max', 'mean', 'median']\n",
        "\n",
        "    # Previous app num features\n",
        "    num_aggregations = {\n",
        "        'AMT_ANNUITY': ['min', 'max', 'mean', 'median'],\n",
        "        'AMT_APPLICATION': ['min', 'max', 'mean', 'median'],\n",
        "        'AMT_CREDIT': ['min', 'max', 'mean', 'median'],\n",
        "        'AMT_DOWN_PAYMENT': ['min', 'max', 'mean', 'median'],\n",
        "        'AMT_GOODS_PRICE': ['min', 'max', 'mean', 'median'],\n",
        "        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean', 'median'],\n",
        "        'DAYS_DECISION': ['min', 'max', 'mean', 'median'],\n",
        "        'CNT_PAYMENT': ['min', 'max', 'mean', 'median'],\n",
        "        'NEW_APP_CREDIT_RATIO': ['min', 'max', 'mean', 'median'],\n",
        "        'NEW_ANNUITY_CREDIT_RATIO': ['min', 'max', 'mean', 'median'],\n",
        "        'NEW_CREDIT_GOODS_RATIO': ['min', 'max', 'mean', 'median'],\n",
        "        'NEW_AMT_INTEREST': ['min', 'max', 'mean', 'median'],\n",
        "        'NEW_INTEREST_RATIO': ['min', 'max', 'mean', 'median'],\n",
        "        'NEW_AMT_NEEDED_CREDIT_RATIO': ['min', 'max', 'mean', 'median'],\n",
        "        'NEW_WEEKDAY_SIN': ['min', 'max', 'mean', 'median'],\n",
        "        'NEW_WEEKDAY_COS': ['min', 'max', 'mean', 'median']}\n",
        "\n",
        "    # Previous app cat features\n",
        "    cat_aggregations = {}\n",
        "    for cat in all_cat_cols:\n",
        "        cat_aggregations[cat] = ['mean']\n",
        "\n",
        "    final_prev_df = new_df_prev.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
        "    final_prev_df.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in final_prev_df.columns.tolist()])\n",
        "\n",
        "    # Approved App - only num features\n",
        "    approved = new_df_prev[new_df_prev['NAME_CONTRACT_STATUS_Approved'] == 1]\n",
        "    approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations)\n",
        "    approved_agg.columns = pd.Index(\n",
        "        ['PREV_APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n",
        "    final_prev_df = final_prev_df.join(approved_agg, how='left', on='SK_ID_CURR')\n",
        "\n",
        "    # refused App - only numerical features\n",
        "    refused = new_df_prev[new_df_prev['NAME_CONTRACT_STATUS_Refused'] == 1]\n",
        "    refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations)\n",
        "    refused_agg.columns = pd.Index(['PREV_REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n",
        "    final_prev_df = final_prev_df.join(refused_agg, how='left', on='SK_ID_CURR')\n",
        "\n",
        "    del refused, refused_agg, approved, approved_agg, new_df_prev\n",
        "    gc.collect()\n",
        "    return final_prev_df\n",
        "\n",
        "\n",
        "#####################################\n",
        "# POS_CASH_balance Data\n",
        "#####################################\n",
        "def pos_cash(num_rows=None, nan_as_category=True):\n",
        "    pos = pd.read_csv('/content/drive/MyDrive/Risk/POS_CASH_balance.csv', nrows=num_rows)\n",
        "    pos, cat_cols = one_hot_encoder(pos, nan_as_category)\n",
        "\n",
        "    # Features\n",
        "    aggregations = {\n",
        "        'MONTHS_BALANCE': ['max', 'mean', 'size'],\n",
        "        'SK_DPD': ['max', 'mean'],\n",
        "        'SK_DPD_DEF': ['max', 'mean']\n",
        "    }\n",
        "    for cat in cat_cols:\n",
        "        aggregations[cat] = ['mean']\n",
        "\n",
        "    pos_agg = pos.groupby('SK_ID_CURR').agg(aggregations)\n",
        "    pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n",
        "\n",
        "    # Count pos cash accounts\n",
        "    pos_agg['POS_COUNT'] = pos.groupby('SK_ID_CURR').size()\n",
        "\n",
        "    del pos\n",
        "    gc.collect()\n",
        "    return pos_agg\n",
        "\n",
        "\n",
        "#####################################\n",
        "# Installments_payments Data\n",
        "#####################################\n",
        "def installments_payments(num_rows=None, nan_as_category=True):\n",
        "    ins = pd.read_csv('/content/drive/MyDrive/Risk/installments_payments.csv', nrows=num_rows)\n",
        "\n",
        "    ins, cat_cols = one_hot_encoder(ins, nan_as_category)\n",
        "\n",
        "    # Percentage and difference paid in each installment (amount paid and installment value)\n",
        "    ins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\n",
        "    ins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n",
        "\n",
        "    # Days past due and days before due (no negative values)\n",
        "    ins['DPD'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\n",
        "    ins['DBD'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\n",
        "    ins['DPD'] = ins['DPD'].apply(lambda x: x if x > 0 else 0)\n",
        "    ins['DBD'] = ins['DBD'].apply(lambda x: x if x > 0 else 0)\n",
        "\n",
        "    # Sayısal ve kategorik sütunları ayıralım\n",
        "    num_cols = [col for col in ins.columns if col not in ['SK_ID_CURR'] + cat_cols]\n",
        "\n",
        "    # Sayısal değişkenler için agregasyonlar\n",
        "    num_aggregations = {\n",
        "        'NUM_INSTALMENT_VERSION': ['nunique'],\n",
        "        'DPD': ['max', 'mean', 'sum'],\n",
        "        'DBD': ['max', 'mean', 'sum'],\n",
        "        'PAYMENT_PERC': ['max', 'mean', 'sum', 'var'],\n",
        "        'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n",
        "        'AMT_INSTALMENT': ['max', 'mean', 'sum'],\n",
        "        'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
        "        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum']\n",
        "    }\n",
        "\n",
        "    # Kategorik değişkenler için agregasyonlar\n",
        "    cat_aggregations = {}\n",
        "    for cat in cat_cols:\n",
        "        cat_aggregations[cat] = ['mean']\n",
        "\n",
        "    # Tüm agregasyonları birleştirelim\n",
        "    ins_agg = ins.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
        "    ins_agg.columns = ['INSTAL_' + '_'.join(col).upper() for col in ins_agg.columns.values]\n",
        "\n",
        "    # Taksit hesaplarını sayalım\n",
        "    ins_agg['INSTAL_COUNT'] = ins.groupby('SK_ID_CURR').size()\n",
        "\n",
        "    del ins\n",
        "    gc.collect()\n",
        "    return ins_agg\n",
        "\n",
        "#####################################\n",
        "# Credit_card_balance Data\n",
        "#####################################\n",
        "def credit_card_balance(num_rows=None, nan_as_category=True):\n",
        "    cc = pd.read_csv('/content/drive/MyDrive/Risk/credit_card_balance.csv', nrows=num_rows)\n",
        "    cc, cat_cols = one_hot_encoder(cc, nan_as_category)\n",
        "\n",
        "    cc.drop(['SK_ID_PREV'], axis=1, inplace=True)\n",
        "\n",
        "    # Sayısal ve kategorik sütunları ayıralım\n",
        "    num_cols = [col for col in cc.columns if col not in ['SK_ID_CURR'] + cat_cols]\n",
        "\n",
        "    # Sayısal değişkenler için agregasyonlar\n",
        "    num_aggregations = {}\n",
        "    for col in num_cols:\n",
        "        num_aggregations[col] = ['min', 'max', 'mean', 'sum', 'var']\n",
        "\n",
        "    # Kategorik değişkenler için agregasyonlar\n",
        "    cat_aggregations = {}\n",
        "    for cat in cat_cols:\n",
        "        cat_aggregations[cat] = ['mean']\n",
        "\n",
        "    # Tüm agregasyonları birleştirelim\n",
        "    cc_agg = cc.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
        "\n",
        "    # Sütun isimlerini düzenleyelim\n",
        "    cc_agg.columns = ['CC_' + '_'.join(col).upper() for col in cc_agg.columns.values]\n",
        "\n",
        "    # Kredi kartı satırlarını sayalım\n",
        "    cc_agg['CC_COUNT'] = cc.groupby('SK_ID_CURR').size()\n",
        "\n",
        "    del cc\n",
        "    gc.collect()\n",
        "    return cc_agg\n",
        "\n",
        "import optuna\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "def optimize_xgb(trial, X_train, y_train, X_valid, y_valid):\n",
        "    params = {\n",
        "        'objective': 'binary:logistic',\n",
        "        'eval_metric': 'auc',\n",
        "        'tree_method': 'gpu_hist',\n",
        "        'use_label_encoder': False,\n",
        "        'seed': 27,\n",
        "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.05),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 500, 1500),\n",
        "        'max_depth': trial.suggest_int('max_depth', 4, 10),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
        "        'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
        "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
        "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 0.5, 3.0),\n",
        "        'scale_pos_weight': trial.suggest_uniform('scale_pos_weight', 1, 5)\n",
        "    }\n",
        "\n",
        "    clf = XGBClassifier(**params)\n",
        "    clf.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_valid, y_valid)],\n",
        "        verbose=False\n",
        "    )\n",
        "    preds = clf.predict_proba(X_valid)[:, 1]\n",
        "    auc = roc_auc_score(y_valid, preds)\n",
        "    return auc\n",
        "\n",
        "def hyperparameter_optimization(model_type, X, y, n_trials=5):\n",
        "    def objective(trial):\n",
        "        skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=27)\n",
        "        aucs = []\n",
        "        for train_idx, valid_idx in skf.split(X, y):\n",
        "            X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
        "            y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
        "            if model_type == 'xgb':\n",
        "                auc = optimize_xgb(trial, X_train, y_train, X_valid, y_valid)\n",
        "            aucs.append(auc)\n",
        "        return np.mean(aucs)\n",
        "\n",
        "    study = optuna.create_study(direction='maximize')\n",
        "    study.optimize(objective, n_trials=n_trials, timeout=600)\n",
        "    print(f\"Best trial for {model_type}: {study.best_trial.params}\")\n",
        "    return study.best_trial.params\n",
        "\n",
        "import optuna\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from catboost import CatBoostClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "def optimize_xgb(trial, X_train, y_train, X_valid, y_valid):\n",
        "    params = {\n",
        "        'objective': 'binary:logistic',\n",
        "        'eval_metric': 'auc',\n",
        "        'tree_method': 'gpu_hist',\n",
        "        'use_label_encoder': False,\n",
        "        'seed': 27,\n",
        "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.05),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 500, 1500),\n",
        "        'max_depth': trial.suggest_int('max_depth', 4, 10),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
        "        'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
        "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
        "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 0.5, 3.0),\n",
        "        'scale_pos_weight': trial.suggest_uniform('scale_pos_weight', 1, 5)\n",
        "    }\n",
        "\n",
        "    clf = XGBClassifier(**params)\n",
        "    clf.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_valid, y_valid)],\n",
        "        verbose=False\n",
        "    )\n",
        "    preds = clf.predict_proba(X_valid)[:, 1]\n",
        "    auc = roc_auc_score(y_valid, preds)\n",
        "    return auc\n",
        "\n",
        "def optimize_lgbm(trial, X_train, y_train, X_valid, y_valid):\n",
        "    params = {\n",
        "        'objective': 'binary',\n",
        "        'metric': 'auc',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'seed': 27,\n",
        "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.05),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 500, 1500),\n",
        "        'max_depth': trial.suggest_int('max_depth', 4, 10),\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
        "        'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
        "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
        "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 0.5, 3.0),\n",
        "        'scale_pos_weight': trial.suggest_uniform('scale_pos_weight', 1, 5)\n",
        "    }\n",
        "\n",
        "    clf = LGBMClassifier(**params)\n",
        "    clf.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_valid, y_valid)],\n",
        "    )\n",
        "    preds = clf.predict_proba(X_valid)[:, 1]\n",
        "    auc = roc_auc_score(y_valid, preds)\n",
        "    return auc\n",
        "\n",
        "def optimize_cat(trial, X_train, y_train, X_valid, y_valid):\n",
        "    params = {\n",
        "        'iterations': trial.suggest_int('iterations', 500, 1500),\n",
        "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.05),\n",
        "        'depth': trial.suggest_int('depth', 4, 10),\n",
        "        'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.5, 3.0),\n",
        "        'scale_pos_weight': trial.suggest_uniform('scale_pos_weight', 1, 5),\n",
        "        'border_count': trial.suggest_int('border_count', 32, 255),\n",
        "        'random_seed': 27,\n",
        "        'logging_level': 'Silent'\n",
        "    }\n",
        "\n",
        "    clf = CatBoostClassifier(**params)\n",
        "    clf.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=(X_valid, y_valid),\n",
        "    )\n",
        "    preds = clf.predict_proba(X_valid)[:, 1]\n",
        "    auc = roc_auc_score(y_valid, preds)\n",
        "    return auc\n",
        "\n",
        "def hyperparameter_optimization(model_type, X, y, n_trials=8):\n",
        "    def objective(trial):\n",
        "        skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=27)\n",
        "        aucs = []\n",
        "        for train_idx, valid_idx in skf.split(X, y):\n",
        "            X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
        "            y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
        "            if model_type == 'xgb':\n",
        "                auc = optimize_xgb(trial, X_train, y_train, X_valid, y_valid)\n",
        "            elif model_type == 'lgbm':\n",
        "                auc = optimize_lgbm(trial, X_train, y_train, X_valid, y_valid)\n",
        "            elif model_type == 'cat':\n",
        "                auc = optimize_cat(trial, X_train, y_train, X_valid, y_valid)\n",
        "            aucs.append(auc)\n",
        "        return np.mean(aucs)\n",
        "\n",
        "    study = optuna.create_study(direction='maximize')\n",
        "    study.optimize(objective, n_trials=n_trials, timeout=600)\n",
        "    print(f\"Best trial for {model_type}: {study.best_trial.params}\")\n",
        "    return study.best_trial.params\n",
        "\n",
        "def train_models(train_df, test_df, feats, n_trials=1):\n",
        "    # Hiperparametre optimizasyonu\n",
        "    print(\"Starting hyperparameter optimization for XGBoost...\")\n",
        "    best_params_xgb = hyperparameter_optimization('xgb', train_df[feats], train_df['TARGET'], n_trials)\n",
        "\n",
        "    print(\"Starting hyperparameter optimization for LightGBM...\")\n",
        "    best_params_lgbm = hyperparameter_optimization('lgbm', train_df[feats], train_df['TARGET'], n_trials)\n",
        "\n",
        "    print(\"Starting hyperparameter optimization for CatBoost...\")\n",
        "    best_params_cat = hyperparameter_optimization('cat', train_df[feats], train_df['TARGET'], n_trials)\n",
        "\n",
        "    # Modellerin Tanımlanması\n",
        "    clf_xgb = XGBClassifier(**best_params_xgb, objective='binary:logistic', eval_metric='auc', tree_method='gpu_hist', use_label_encoder=False, seed=27, n_jobs=-1)\n",
        "    clf_lgbm = LGBMClassifier(**best_params_lgbm, objective='binary', metric='auc', seed=27, n_jobs=-1)\n",
        "    clf_cat = CatBoostClassifier(**best_params_cat, task_type='GPU', verbose=False, random_seed=27)\n",
        "\n",
        "    # Ensemble Tahminleri\n",
        "    oof_preds_xgb = np.zeros(train_df.shape[0])\n",
        "    oof_preds_lgbm = np.zeros(train_df.shape[0])\n",
        "    oof_preds_cat = np.zeros(train_df.shape[0])\n",
        "\n",
        "    sub_preds_xgb = np.zeros(test_df.shape[0])\n",
        "    sub_preds_lgbm = np.zeros(test_df.shape[0])\n",
        "    sub_preds_cat = np.zeros(test_df.shape[0])\n",
        "\n",
        "    feature_importance_df = pd.DataFrame()\n",
        "\n",
        "    # Cross-validation setup\n",
        "    folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=27)\n",
        "\n",
        "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n",
        "        print(f\"Training fold {n_fold + 1}\")\n",
        "        train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]\n",
        "        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]\n",
        "\n",
        "        # XGBoost\n",
        "        clf_xgb.fit(\n",
        "            train_x, train_y,\n",
        "            eval_set=[(valid_x, valid_y)],\n",
        "        )\n",
        "        oof_preds_xgb[valid_idx] = clf_xgb.predict_proba(valid_x)[:, 1]\n",
        "        sub_preds_xgb += clf_xgb.predict_proba(test_df[feats])[:, 1] / folds.n_splits\n",
        "\n",
        "        # Feature importance for XGBoost\n",
        "        fold_importance_xgb = pd.DataFrame()\n",
        "        fold_importance_xgb[\"feature\"] = feats\n",
        "        fold_importance_xgb[\"importance\"] = clf_xgb.feature_importances_\n",
        "        fold_importance_xgb[\"fold\"] = n_fold + 1\n",
        "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_xgb], axis=0)\n",
        "        # LightGBM\n",
        "        clf_lgbm.fit(\n",
        "            train_x, train_y,\n",
        "            eval_set=[(valid_x, valid_y)],\n",
        "        )\n",
        "        oof_preds_lgbm[valid_idx] = clf_lgbm.predict_proba(valid_x)[:, 1]\n",
        "        sub_preds_lgbm += clf_lgbm.predict_proba(test_df[feats])[:, 1] / folds.n_splits\n",
        "\n",
        "        # Feature importance for LightGBM\n",
        "        fold_importance_lgbm = pd.DataFrame()\n",
        "        fold_importance_lgbm[\"feature\"] = feats\n",
        "        fold_importance_lgbm[\"importance\"] = clf_lgbm.feature_importances_\n",
        "        fold_importance_lgbm[\"fold\"] = n_fold + 1\n",
        "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_lgbm], axis=0)\n",
        "\n",
        "        # CatBoost\n",
        "        clf_cat.fit(\n",
        "            train_x, train_y,\n",
        "            eval_set=[(valid_x, valid_y)],\n",
        "        )\n",
        "        oof_preds_cat[valid_idx] = clf_cat.predict_proba(valid_x)[:, 1]\n",
        "        sub_preds_cat += clf_cat.predict_proba(test_df[feats])[:, 1] / folds.n_splits\n",
        "\n",
        "        # Feature importance for CatBoost\n",
        "        fold_importance_cat = pd.DataFrame()\n",
        "        fold_importance_cat[\"feature\"] = feats\n",
        "        fold_importance_cat[\"importance\"] = clf_cat.get_feature_importance()\n",
        "        fold_importance_cat[\"fold\"] = n_fold + 1\n",
        "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_cat], axis=0)\n",
        "\n",
        "        # AUC Skorları\n",
        "        auc_xgb = roc_auc_score(valid_y, oof_preds_xgb[valid_idx])\n",
        "        auc_lgbm = roc_auc_score(valid_y, oof_preds_lgbm[valid_idx])\n",
        "        auc_cat = roc_auc_score(valid_y, oof_preds_cat[valid_idx])\n",
        "        print(f'Fold {n_fold + 1} AUC XGB: {auc_xgb:.6f}, LGBM: {auc_lgbm:.6f}, CatBoost: {auc_cat:.6f}')\n",
        "\n",
        "        # Bellek temizleme\n",
        "        del train_x, train_y, valid_x, valid_y\n",
        "        gc.collect()\n",
        "\n",
        "    # Ensemble tahminlerini ortalama\n",
        "    oof_preds = (oof_preds_xgb + oof_preds_lgbm + oof_preds_cat) / 3\n",
        "    full_auc = roc_auc_score(train_df['TARGET'], oof_preds)\n",
        "    print(f'Full AUC score: {full_auc:.6f}')\n",
        "\n",
        "    # Ensemble tahminleri test seti için ortalama\n",
        "    sub_preds = (sub_preds_xgb + sub_preds_lgbm + sub_preds_cat) / 3\n",
        "    test_df['TARGET'] = sub_preds\n",
        "\n",
        "    # Tahminleri kaydetme\n",
        "    test_df[['SK_ID_CURR', 'TARGET']].to_csv(submission_file_name, index=False)\n",
        "    print(\"Submission file saved.\")\n",
        "\n",
        "    # Özellik önemlerini gösterme\n",
        "    display_importances(feature_importance_df)\n",
        "\n",
        "    # SHAP ile model açıklanabilirliği (isteğe bağlı)\n",
        "    import shap\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # SHAP değerlerini hesaplamak için en iyi XGBoost modelini kullanıyoruz\n",
        "    explainer = shap.Explainer(clf_xgb)\n",
        "    shap_values = explainer(train_df[feats])\n",
        "    shap.summary_plot(shap_values, train_df[feats], show=False)\n",
        "    plt.savefig('shap_summary_plot.png')\n",
        "    plt.close()\n",
        "\n",
        "    return feature_importance_df\n",
        "\n",
        "def clean_feature_names(df):\n",
        "    \"\"\"\n",
        "    Temizle özelliği: LightGBM'in desteklemediği özel karakterleri kaldırır.\n",
        "    \"\"\"\n",
        "    df.columns = df.columns.str.replace(r'[^A-Za-z0-9_]+', '_', regex=True)\n",
        "    return df\n",
        "\n",
        "#####################################\n",
        "# Main Function\n",
        "#####################################\n",
        "def main(debug=False):\n",
        "    num_rows = 500000 if debug else None\n",
        "    with timer(\"Veri Yükleme ve Ön İşleme\"):\n",
        "        df = application_train_test(num_rows)\n",
        "\n",
        "    with timer(\"Process bureau and bureau_balance\"):\n",
        "        bureau = bureau_and_balance(num_rows)\n",
        "        print(\"Bureau df shape:\", bureau.shape)\n",
        "        df = df.join(bureau, how='left', on='SK_ID_CURR')\n",
        "        del bureau\n",
        "        gc.collect()\n",
        "\n",
        "    with timer(\"Process previous_applications\"):\n",
        "        prev = previous_app(num_rows)\n",
        "        print(\"Previous applications df shape:\", prev.shape)\n",
        "        df = df.join(prev, how='left', on='SK_ID_CURR')\n",
        "        del prev\n",
        "        gc.collect()\n",
        "\n",
        "    with timer(\"Process POS-CASH balance\"):\n",
        "        pos = pos_cash(num_rows)\n",
        "        print(\"Pos-cash balance df shape:\", pos.shape)\n",
        "        df = df.join(pos, how='left', on='SK_ID_CURR')\n",
        "        del pos\n",
        "        gc.collect()\n",
        "\n",
        "    with timer(\"Process installments payments\"):\n",
        "        ins = installments_payments(num_rows)\n",
        "        print(\"Installments payments df shape:\", ins.shape)\n",
        "        df = df.join(ins, how='left', on='SK_ID_CURR')\n",
        "        del ins\n",
        "        gc.collect()\n",
        "\n",
        "    with timer(\"Process credit card balance\"):\n",
        "        cc = credit_card_balance(num_rows)\n",
        "        print(\"Credit card balance df shape:\", cc.shape)\n",
        "        df = df.join(cc, how='left', on='SK_ID_CURR')\n",
        "        # Exporting combined_df to investigate features\n",
        "        df.to_csv('combined_df.csv')\n",
        "        del cc\n",
        "        gc.collect()\n",
        "\n",
        "    # Özellik isimlerini temizle\n",
        "    df = clean_feature_names(df)\n",
        "\n",
        "    # Model Eğitimine Hazırlık\n",
        "    with timer(\"Prepare data for modeling\"):\n",
        "        train_df = df[df['TARGET'].notnull()]\n",
        "        test_df = df[df['TARGET'].isnull()]\n",
        "        feats = [f for f in train_df.columns if f not in ['TARGET', 'SK_ID_CURR', 'SK_ID_BUREAU', 'SK_ID_PREV', 'index']]\n",
        "\n",
        "        # Inf ve NaN değerlerini kontrol et ve temizle\n",
        "        train_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "        test_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "        # NaN değerlerini median ile doldur\n",
        "        train_df.fillna(train_df.median(), inplace=True)\n",
        "        test_df.fillna(test_df.median(), inplace=True)\n",
        "\n",
        "        del df\n",
        "        gc.collect()\n",
        "\n",
        "    with timer(\"Run Ensemble Models\"):\n",
        "        feature_importance_df = train_models(train_df, test_df, feats, n_trials=5)\n",
        "\n",
        "    print(\"Model training completed.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    submission_file_name = \"/content/drive/MyDrive/Risk/submission_DSMLBC4_Grp2.csv\"\n",
        "    with timer(\"Full model run\"):\n",
        "        main(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dolandiricilik_tespiti.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "data = kagglehub.dataset_download(\"mlg-ulb/creditcardfraud\")\n",
        "\n",
        "print(\"Path to dataset files:\", data)\n",
        "\n",
        "def load_data():\n",
        "    data = kagglehub.dataset_download(\"mlg-ulb/creditcardfraud\")\n",
        "    print(\"Path to dataset files:\", data)\n",
        "\n",
        "def preprocess_data(data):\n",
        "    data.fillna(0, inplace=True)\n",
        "    X = data.drop('Class', axis=1)\n",
        "    y = data['Class']\n",
        "    return X, y\n",
        "\n",
        "def scale_data(X):\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    return X_scaled, scaler\n",
        "\n",
        "def build_autoencoder(input_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(32, activation='relu', input_dim=input_dim))\n",
        "    model.add(Dense(16, activation='relu'))\n",
        "    model.add(Dense(8, activation='relu'))\n",
        "    model.add(Dense(16, activation='relu'))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(input_dim, activation='linear'))\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n",
        "\n",
        "def main():\n",
        "    data = load_data()\n",
        "    X, y = preprocess_data(data)\n",
        "    X_scaled, scaler = scale_data(X)\n",
        "    # Normal işlemlerle eğitme\n",
        "    X_train = X_scaled[y == 0]\n",
        "    X_test = X_scaled\n",
        "    y_test = y\n",
        "    autoencoder = build_autoencoder(X_train.shape[1])\n",
        "    autoencoder.fit(X_train, X_train, epochs=10, batch_size=32, shuffle=True, validation_split=0.1)\n",
        "    reconstructions = autoencoder.predict(X_test)\n",
        "    mse = np.mean(np.power(X_test - reconstructions, 2), axis=1)\n",
        "    error_df = pd.DataFrame({'reconstruction_error': mse, 'true_class': y_test})\n",
        "    threshold = error_df[error_df['true_class'] == 0]['reconstruction_error'].quantile(0.99)\n",
        "    y_pred = [1 if e > threshold else 0 for e in error_df['reconstruction_error'].values]\n",
        "    print(\"ROC AUC Skoru:\", roc_auc_score(y_test, y_pred))\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d')\n",
        "    plt.show()\n",
        "    autoencoder.save('models/fraud_detection_autoencoder.h5')\n",
        "    joblib.dump(scaler, 'models/fraud_scaler.pkl')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "yiGLIzaRbLPM",
        "outputId": "161aa850-6776-4b22-dd10-571f3e1dc9d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/mlg-ulb/creditcardfraud?dataset_version_number=3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66.0M/66.0M [00:03<00:00, 17.5MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/mlg-ulb/creditcardfraud/versions/3\n",
            "Path to dataset files: /root/.cache/kagglehub/datasets/mlg-ulb/creditcardfraud/versions/3\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'fillna'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-51ca3844473c>\u001b[0m in \u001b[0;36m<cell line: 69>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-51ca3844473c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mX_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscale_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# Normal işlemlerle eğitme\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-51ca3844473c>\u001b[0m in \u001b[0;36mpreprocess_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Class'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'fillna'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# musteri_segmentasyonu.py\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.decomposition import PCA\n",
        "import plotly.express as px\n",
        "import joblib\n",
        "\n",
        "def load_data():\n",
        "    data = pd.read_csv('bank_marketing.csv')\n",
        "    return data\n",
        "\n",
        "def preprocess_data(data):\n",
        "    data.fillna(method='ffill', inplace=True)\n",
        "    categorical_cols = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n",
        "    data = pd.get_dummies(data, columns=categorical_cols)\n",
        "    return data\n",
        "\n",
        "def scale_data(data):\n",
        "    scaler = StandardScaler()\n",
        "    data_scaled = scaler.fit_transform(data)\n",
        "    return data_scaled, scaler\n",
        "\n",
        "def reduce_dimensions(data_scaled):\n",
        "    pca = PCA(n_components=3)\n",
        "    data_pca = pca.fit_transform(data_scaled)\n",
        "    return data_pca, pca\n",
        "\n",
        "def cluster_data(data_pca):\n",
        "    model = AgglomerativeClustering(n_clusters=4)\n",
        "    clusters = model.fit_predict(data_pca)\n",
        "    return clusters, model\n",
        "\n",
        "def visualize_clusters(data_pca, clusters):\n",
        "    fig = px.scatter_3d(x=data_pca[:,0], y=data_pca[:,1], z=data_pca[:,2], color=clusters.astype(str))\n",
        "    fig.show()\n",
        "\n",
        "def save_models(scaler, pca, model):\n",
        "    joblib.dump(scaler, 'models/segment_scaler.pkl')\n",
        "    joblib.dump(pca, 'models/segment_pca.pkl')\n",
        "    joblib.dump(model, 'models/segment_model.pkl')\n",
        "\n",
        "def main():\n",
        "    data = load_data()\n",
        "    data_processed = preprocess_data(data)\n",
        "    data_scaled, scaler = scale_data(data_processed)\n",
        "    data_pca, pca = reduce_dimensions(data_scaled)\n",
        "    clusters, model = cluster_data(data_pca)\n",
        "    data['segment'] = clusters\n",
        "    silhouette_avg = silhouette_score(data_scaled, clusters)\n",
        "    print(\"Silhouette Score:\", silhouette_avg)\n",
        "    visualize_clusters(data_pca, clusters)\n",
        "    data.to_csv('customer_segments.csv', index=False)\n",
        "    save_models(scaler, pca, model)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "fcYAE5HCbNfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gelir_tahmini.py\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "import optuna\n",
        "import joblib\n",
        "import kagglehub\n",
        "\n",
        "def load_data():\n",
        "    # Download latest version\n",
        "    path = kagglehub.dataset_download(\"kamaumunyori/income-prediction-dataset-us-20th-century-data\")\n",
        "\n",
        "    print(\"Path to dataset files:\", path)\n",
        "\n",
        "def preprocess_data(data):\n",
        "    data.fillna(method='ffill', inplace=True)\n",
        "    categorical_cols = data.select_dtypes(include=['object']).columns\n",
        "    data = pd.get_dummies(data, columns=categorical_cols)\n",
        "    X = data.drop('income', axis=1)\n",
        "    y = data['income']\n",
        "    return X, y\n",
        "\n",
        "def scale_data(X):\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    return X_scaled, scaler\n",
        "\n",
        "def build_model(trial, input_dim):\n",
        "    model = Sequential()\n",
        "    n_layers = trial.suggest_int('n_layers', 1, 5)\n",
        "    for i in range(n_layers):\n",
        "        num_units = trial.suggest_int('n_units_l{}'.format(i), 32, 256)\n",
        "        model.add(Dense(num_units, activation='relu'))\n",
        "        dropout_rate = trial.suggest_float('dropout_l{}'.format(i), 0.0, 0.5)\n",
        "        model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "def optimize_model(X_train, y_train, input_dim):\n",
        "    def objective(trial):\n",
        "        model = build_model(trial, input_dim)\n",
        "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=5)\n",
        "        history = model.fit(X_train, y_train, validation_split=0.1, callbacks=[es], epochs=50, batch_size=32, verbose=0)\n",
        "        loss = history.history['val_loss'][-1]\n",
        "        return loss\n",
        "    study = optuna.create_study(direction='minimize')\n",
        "    study.optimize(objective, n_trials=20)\n",
        "    return study.best_params\n",
        "\n",
        "def train_model(X_train, y_train, params, input_dim):\n",
        "    model = build_model(optuna.trial.FixedTrial(params), input_dim)\n",
        "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
        "    model.fit(X_train, y_train, validation_split=0.1, callbacks=[es], epochs=100, batch_size=32, verbose=1)\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    print(\"Mean Absolute Error:\", mae)\n",
        "\n",
        "def main():\n",
        "    data = load_data()\n",
        "    X, y = preprocess_data(data)\n",
        "    X_scaled, scaler = scale_data(X)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "    input_dim = X_train.shape[1]\n",
        "    best_params = optimize_model(X_train, y_train, input_dim)\n",
        "    model = train_model(X_train, y_train, best_params, input_dim)\n",
        "    evaluate_model(model, X_test, y_test)\n",
        "    model.save('models/income_prediction_model.h5')\n",
        "    joblib.dump(scaler, 'models/income_scaler.pkl')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "AVnGcD8AbQkf",
        "outputId": "a565fcd5-6510-4d42-cbad-4cdddb20b4d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/kamaumunyori/income-prediction-dataset-us-20th-century-data?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.02M/9.02M [00:01<00:00, 5.86MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/kamaumunyori/income-prediction-dataset-us-20th-century-data/versions/1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'fillna'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-07f7d33abb0f>\u001b[0m in \u001b[0;36m<cell line: 80>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-07f7d33abb0f>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0mX_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscale_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-07f7d33abb0f>\u001b[0m in \u001b[0;36mpreprocess_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ffill'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mcategorical_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'fillna'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# finansal_saglik_skoru.py\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import kagglehub\n",
        "def load_data():\n",
        "    # Download latest version\n",
        "    data = kagglehub.dataset_download(\"teertha/personal-loan-modeling\")\n",
        "\n",
        "    print(\"Path to dataset files:\", data)\n",
        "\n",
        "def preprocess_data(data):\n",
        "    data.fillna(method='ffill', inplace=True)\n",
        "    return data\n",
        "\n",
        "def calculate_scores(data):\n",
        "    data['DebtIncomeRatio'] = data['TotalDebt'] / data['AnnualIncome']\n",
        "    data['CreditUtilization'] = data['CurrentBalance'] / data['CreditLimit']\n",
        "    data['PaymentHistoryScore'] = data['OnTimePayments'] / data['TotalPayments']\n",
        "    data['LengthOfCreditHistory'] = data['CreditHistoryYears']\n",
        "    # Özellikleri normalleştirme\n",
        "    scaler = MinMaxScaler()\n",
        "    features = ['DebtIncomeRatio', 'CreditUtilization', 'PaymentHistoryScore', 'LengthOfCreditHistory']\n",
        "    data[features] = scaler.fit_transform(data[features])\n",
        "    # Ağırlıklı skor hesaplama\n",
        "    weights = {'DebtIncomeRatio': 0.3, 'CreditUtilization': 0.3, 'PaymentHistoryScore': 0.3, 'LengthOfCreditHistory': 0.1}\n",
        "    data['FinancialHealthScore'] = (data['DebtIncomeRatio'] * weights['DebtIncomeRatio'] +\n",
        "                                    data['CreditUtilization'] * weights['CreditUtilization'] +\n",
        "                                    data['PaymentHistoryScore'] * weights['PaymentHistoryScore'] +\n",
        "                                    data['LengthOfCreditHistory'] * weights['LengthOfCreditHistory']) * 100\n",
        "    return data\n",
        "\n",
        "def main():\n",
        "    data = load_data()\n",
        "    data = preprocess_data(data)\n",
        "    data = calculate_scores(data)\n",
        "    data.to_csv('financial_health_scores.csv', index=False)\n",
        "    print(\"Finansal Sağlık Skoru Hesaplandı ve Kaydedildi.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "JoTIYGNkbR_V",
        "outputId": "671665a3-8a9a-4473-c163-4cc744b10440"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/teertha/personal-loan-modeling/versions/1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'NoneType' object is not subscriptable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-621ca0405818>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-621ca0405818>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'financial_health_scores.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Finansal Sağlık Skoru Hesaplandı ve Kaydedildi.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-621ca0405818>\u001b[0m in \u001b[0;36mcalculate_scores\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcalculate_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DebtIncomeRatio'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TotalDebt'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AnnualIncome'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CreditUtilization'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CurrentBalance'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CreditLimit'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'PaymentHistoryScore'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'OnTimePayments'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TotalPayments'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# crm_entegrasyonu.py\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def load_data():\n",
        "    crm_data = pd.read_csv('crm_data.csv')\n",
        "    segments = pd.read_csv('customer_segments.csv')\n",
        "    return crm_data, segments\n",
        "\n",
        "def merge_data(crm_data, segments):\n",
        "    data = crm_data.merge(segments[['customer_id', 'segment']], on='customer_id', how='left')\n",
        "    return data\n",
        "\n",
        "def analyze_behavior(data):\n",
        "    behavior = data.groupby('segment').agg({\n",
        "        'purchase_amount': ['mean', 'sum'],\n",
        "        'interaction_count': 'mean',\n",
        "        'customer_lifetime_value': 'mean'\n",
        "    })\n",
        "    behavior.columns = ['_'.join(col).strip() for col in behavior.columns.values]\n",
        "    behavior.reset_index(inplace=True)\n",
        "    behavior.to_csv('segment_behavior.csv', index=False)\n",
        "    return behavior\n",
        "\n",
        "def main():\n",
        "    crm_data, segments = load_data()\n",
        "    data = merge_data(crm_data, segments)\n",
        "    behavior = analyze_behavior(data)\n",
        "    print(\"Segment Davranış Analizi Tamamlandı ve Kaydedildi.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "YZGA-hUKbUAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# raporlama_dashboard.py\n",
        "\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import dash\n",
        "from dash import dcc, html\n",
        "from dash.dependencies import Input, Output\n",
        "\n",
        "def load_data():\n",
        "    financial_data = pd.read_csv('financial_analytics.csv')\n",
        "    return financial_data\n",
        "\n",
        "def create_dashboard(financial_data):\n",
        "    app = dash.Dash(__name__)\n",
        "\n",
        "    app.layout = html.Div(children=[\n",
        "        html.H1(children='Finansal Analitik Dashboard'),\n",
        "        dcc.Dropdown(\n",
        "            id='metric-dropdown',\n",
        "            options=[\n",
        "                {'label': 'Gelir', 'value': 'income'},\n",
        "                {'label': 'Varlıklar', 'value': 'assets'},\n",
        "                {'label': 'Yükümlülükler', 'value': 'liabilities'}\n",
        "            ],\n",
        "            value='income'\n",
        "        ),\n",
        "        dcc.Graph(id='metric-graph')\n",
        "    ])\n",
        "\n",
        "    @app.callback(\n",
        "        Output('metric-graph', 'figure'),\n",
        "        [Input('metric-dropdown', 'value')]\n",
        "    )\n",
        "    def update_graph(selected_metric):\n",
        "        fig = px.line(financial_data, x='month', y=selected_metric, title=f'Aylık {selected_metric.capitalize()} Trendleri')\n",
        "        return fig\n",
        "\n",
        "    app.run_server(debug=True)\n",
        "\n",
        "def main():\n",
        "    financial_data = load_data()\n",
        "    create_dashboard(financial_data)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "NHkidtE2bVOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all_in_banker_main.py\n",
        "\n",
        "def main():\n",
        "    print(\"ALL-IN-BANKER Platformu Başlatılıyor...\\n\")\n",
        "    import kredi_skorlamasi\n",
        "    kredi_skorlamasi.main()\n",
        "    import dolandiricilik_tespiti\n",
        "    dolandiricilik_tespiti.main()\n",
        "    import musteri_segmentasyonu\n",
        "    musteri_segmentasyonu.main()\n",
        "    import gelir_tahmini\n",
        "    gelir_tahmini.main()\n",
        "    import finansal_saglik_skoru\n",
        "    finansal_saglik_skoru.main()\n",
        "    import crm_entegrasyonu\n",
        "    crm_entegrasyonu.main()\n",
        "    import raporlama_dashboard\n",
        "    raporlama_dashboard.main()\n",
        "    print(\"\\nALL-IN-BANKER Platformu Başarıyla Çalıştırıldı.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "zeP5F-p5bXqr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}